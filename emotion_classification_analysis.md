# ê°ì„± ë¶„ì„ ëª¨ë¸: Yelp ë¦¬ë·° ê¸ì •/ë¶€ì • ë¶„ë¥˜

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

Yelp ë¦¬ë·° ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì˜ ê°ì„±ì„ ê¸ì •/ë¶€ì •ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤. ë³„ì  ê¸°ë°˜ì˜ ëª…í™•í•œ ë¼ë²¨ë§ì„ í†µí•´ ë†’ì€ ì •í™•ë„ì˜ ê°ì„± ë¶„ì„ì´ ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤.

## ğŸ¯ ë¶„ë¥˜ ê¸°ì¤€

- **ê¸ì • (Positive)**: ë³„ì  3-5ì 
- **ë¶€ì • (Negative)**: ë³„ì  1-2ì 
- **ì´ì§„ ë¶„ë¥˜**: 0 (ë¶€ì •) / 1 (ê¸ì •)

## ğŸ“Š ë°ì´í„°ì…‹ ì •ë³´

- **ì „ì²´ ë°ì´í„°**: 560,000ê°œ í›ˆë ¨ ìƒ˜í”Œ, 38,000ê°œ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ
- **ì‚¬ìš© ë°ì´í„°**: ì „ì²´ì˜ 10% (5,000ê°œ ìƒ˜í”Œ)
- **ë°ì´í„° ë¶„í• **: í›ˆë ¨ 80% / ê²€ì¦ 10% / í…ŒìŠ¤íŠ¸ 10%
- **ë°ì´í„° ì†ŒìŠ¤**: Yelp Academic Dataset

## ğŸ§  ëª¨ë¸ ì•„í‚¤í…ì²˜

### 1. ì „ì²´ êµ¬ì¡°

ì´ ëª¨ë¸ì€ **ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ (Single Perceptron)** êµ¬ì¡°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

```
ì…ë ¥ í…ìŠ¤íŠ¸ â†’ ì „ì²˜ë¦¬ â†’ ì›-í•« ë²¡í„° â†’ ì„ í˜• ë³€í™˜ â†’ ì‹œê·¸ëª¨ì´ë“œ â†’ ë¶„ë¥˜ ê²°ê³¼
```

### 2. í•µì‹¬ êµ¬ì„± ìš”ì†Œ

#### 2.1 í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬

**í•µì‹¬ ì „ì²˜ë¦¬ í•¨ìˆ˜:**
```python
import spacy
import re

# spaCy ëª¨ë¸ ë¡œë“œ
nlp = spacy.load("en_core_web_sm")

def preprocess_text_spacy(text):
    """spaCyë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
    doc = nlp(text.lower())  # ì†Œë¬¸ì ë³€í™˜ + í† í°í™”
    out = []
    for tok in doc:
        if tok.is_punct:  # êµ¬ë‘ì  ì²˜ë¦¬
            out.append(f" {tok.text} ")
        elif tok.is_alpha:  # ì•ŒíŒŒë²³ë§Œ ì¶”ì¶œ
            out.append(f" {tok.text} ")
    s = "".join(out)
    s = re.sub(r"\s+", " ", s).strip()  # ê³µë°± ì •ë¦¬
    return s
```

**ì²˜ë¦¬ ê³¼ì •:**
1. **ì†Œë¬¸ì ë³€í™˜**: `text.lower()`
2. **í† í°í™”**: spaCyì˜ ì–¸ì–´ ëª¨ë¸ ì‚¬ìš©
3. **êµ¬ë‘ì  ì²˜ë¦¬**: ë¬¸ì¥ë¶€í˜¸ë¥¼ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
4. **ì•ŒíŒŒë²³ í•„í„°ë§**: ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ì œê±°
5. **ê³µë°± ì •ë¦¬**: ì—°ì†ëœ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©

**ì˜ˆì‹œ:**
- ì…ë ¥: `"This restaurant is AMAZING!!! The food was delicious."`
- ì¶œë ¥: `"this restaurant is amazing the food was delicious"`

#### 2.2 ì–´íœ˜ ì‚¬ì „ êµ¬ì¶•

**Vocabulary í´ë˜ìŠ¤:**
```python
from collections import Counter
import string

class Vocabulary(object):
    def __init__(self, token_to_idx=None, add_unk=True, unk_token="<UNK>"):
        self._token_to_idx = dict(token_to_idx) if token_to_idx else {}
        self._idx_to_token = {i: t for t, i in self._token_to_idx.items()}
        self._add_unk = add_unk
        self._unk_token = unk_token
        self._unk_index = None
        
        if self._add_unk:
            self._unk_index = self.add_token(self._unk_token)
    
    def add_token(self, token):
        if token in self._token_to_idx:
            return self._token_to_idx[token]
        idx = len(self._token_to_idx)
        self._token_to_idx[token] = idx
        self._idx_to_token[idx] = token
        return idx
    
    def lookup_token(self, token):
        return self._token_to_idx.get(token, self._unk_index)
    
    def lookup_index(self, index):
        return self._idx_to_token.get(index, self._unk_token)
```

**ì–´íœ˜ ì‚¬ì „ êµ¬ì¶• ê³¼ì •:**
```python
# 1. ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
word_counts = Counter()
for review in review_df.text:
    for word in review.split(" "):
        if word not in string.punctuation:
            word_counts[word] += 1

# 2. ë¹ˆë„ ê¸°ë°˜ í•„í„°ë§ (cutoff=25)
review_vocab = Vocabulary()
for word, count in word_counts.items():
    if count > cutoff:  # 25ë²ˆ ì´ìƒ ë“±ì¥í•œ ë‹¨ì–´ë§Œ
        review_vocab.add_token(word)
```

**í•µì‹¬ íŠ¹ì§•:**
- **ë¹ˆë„ í•„í„°ë§**: 25íšŒ ì´ìƒ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë§Œ í¬í•¨
- **UNK í† í°**: ë¯¸ì§€ ë‹¨ì–´ ì²˜ë¦¬
- **ì–‘ë°©í–¥ ë§¤í•‘**: ë‹¨ì–´â†”ì¸ë±ìŠ¤ ë³€í™˜ ì§€ì›

#### 2.3 ì›-í•« ì¸ì½”ë”©

**ReviewVectorizer í´ë˜ìŠ¤:**
```python
import numpy as np
import string

class ReviewVectorizer(object):
    def __init__(self, review_vocab):
        self.review_vocab = review_vocab
    
    def vectorize(self, review):
        """í…ìŠ¤íŠ¸ë¥¼ ì›-í•« ë²¡í„°ë¡œ ë³€í™˜"""
        one_hot = np.zeros(len(self.review_vocab), dtype=np.float64)
        for token in review.split(" "):
            if token not in string.punctuation:
                word_index = self.review_vocab.lookup_token(token)
                one_hot[word_index] = 1  # í•´ë‹¹ ìœ„ì¹˜ì— 1 í• ë‹¹
        return one_hot
    
    @classmethod
    def from_dataframe(cls, review_df, cutoff=25):
        """ë°ì´í„°í”„ë ˆì„ì—ì„œ ë²¡í„°ë¼ì´ì € ìƒì„±"""
        review_vocab = Vocabulary()
        
        # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
        word_counts = Counter()
        for review in review_df.text:
            for word in review.split(" "):
                if word not in string.punctuation:
                    word_counts[word] += 1
        
        # ë¹ˆë„ ê¸°ë°˜ í•„í„°ë§
        for word, count in word_counts.items():
            if count > cutoff:
                review_vocab.add_token(word)
        
        return cls(review_vocab)
```

**ë²¡í„°í™” ê³¼ì •:**
```python
# ì–´íœ˜ ì‚¬ì „: {"pizza": 0, "fantastic": 1, "terrible": 2, "good": 3, "bad": 4}
# ë¬¸ì¥: "pizza fantastic good"
# ì›-í•« ë²¡í„°: [1, 1, 0, 1, 0]
```

**í•µì‹¬ íŠ¹ì§•:**
- **í¬ì†Œ ë²¡í„°**: ëŒ€ë¶€ë¶„ 0, ì¼ë¶€ë§Œ 1ì¸ ë²¡í„°
- **ê³ ì°¨ì›**: ì–´íœ˜ ì‚¬ì „ í¬ê¸°ë§Œí¼ì˜ ì°¨ì›
- **ë‹¨ì–´ ì¡´ì¬ ì—¬ë¶€**: ë‹¨ì–´ê°€ ìˆìœ¼ë©´ 1, ì—†ìœ¼ë©´ 0

#### 2.4 ëª¨ë¸ êµ¬ì¡°
```python
class ReviewClassifier(nn.Module):
    def __init__(self, num_features):
        super(ReviewClassifier, self).__init__()
        self.fc1 = nn.Linear(in_features=num_features, out_features=1)
    
    def forward(self, x_in, apply_sigmoid=False):
        y_out = self.fc1(x_in).squeeze()
        if apply_sigmoid:
            y_out = torch.sigmoid(y_out)
        return y_out
```

**ëª¨ë¸ íŠ¹ì§•:**
- **ì…ë ¥ì¸µ**: ì–´íœ˜ ì‚¬ì „ í¬ê¸°ë§Œí¼ì˜ ì›-í•« ë²¡í„°
- **ì¶œë ¥ì¸µ**: 1ê°œ ë‰´ëŸ° (ì´ì§„ ë¶„ë¥˜)
- **í™œì„±í™” í•¨ìˆ˜**: ì‹œê·¸ëª¨ì´ë“œ (í™•ë¥  ì¶œë ¥)

## ğŸ§  ëª¨ë¸ í•™ìŠµ ì›ë¦¬

### 1. í•™ìŠµ ê³¼ì •

**í•µì‹¬ í•™ìŠµ ì½”ë“œ:**
```python
import torch
import torch.nn as nn
import torch.optim as optim

# ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
loss_func = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(classifier.parameters(), lr=0.001)

# í•™ìŠµ ë£¨í”„
for epoch_index in range(num_epochs):
    dataset.set_split('train')
    batch_generator = generate_batches(dataset, batch_size=128, device=device)
    
    for batch_index, batch_dict in enumerate(batch_generator):
        # 1. ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        optimizer.zero_grad()
        
        # 2. ìˆœì „íŒŒ
        y_pred = classifier(x_in=batch_dict['X_data'].float())
        
        # 3. ì†ì‹¤ ê³„ì‚°
        loss = loss_func(y_pred, batch_dict['Y_target'].float())
        
        # 4. ì—­ì „íŒŒ
        loss.backward()
        
        # 5. ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
        optimizer.step()
```

ì´ ëª¨ë¸ì€ **ì§€ë„í•™ìŠµ(Supervised Learning)**ì„ í†µí•´ í•™ìŠµë©ë‹ˆë‹¤:

1. **ìˆœì „íŒŒ**: ì…ë ¥ í…ìŠ¤íŠ¸ â†’ ì›-í•« ë²¡í„° â†’ ì„ í˜• ë³€í™˜ â†’ ì‹œê·¸ëª¨ì´ë“œ â†’ ì˜ˆì¸¡ê°’
2. **ì†ì‹¤ ê³„ì‚°**: BCEWithLogitsLossë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ë¼ë²¨ ë¹„êµ
3. **ì—­ì „íŒŒ**: ì†ì‹¤ì— ëŒ€í•œ ê¸°ìš¸ê¸° ê³„ì‚°
4. **ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸**: Adam ì˜µí‹°ë§ˆì´ì €ë¡œ ê°€ì¤‘ì¹˜ ì¡°ì •

### 2. ê°€ì¤‘ì¹˜ í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜

#### ì´ˆê¸° ìƒíƒœ (ëœë¤ ê°€ì¤‘ì¹˜)
```
ë‹¨ì–´ë³„ ê°€ì¤‘ì¹˜: {"pizza": 0.1, "fantastic": -0.3, "terrible": 0.2, "good": -0.1, "bad": 0.4}
```

#### í•™ìŠµ ê³¼ì •
1. **ì˜ˆì¸¡**: ê¸ì • ë¦¬ë·° "pizza fantastic good" â†’ ê°€ì¤‘ì¹˜ í•©ê³„ = 0.1 + (-0.3) + (-0.1) = -0.3
2. **í™•ë¥  ë³€í™˜**: ì‹œê·¸ëª¨ì´ë“œ(-0.3) = 0.43 < 0.5 â†’ "ë¶€ì •" (í‹€ë¦¼!)
3. **ì†ì‹¤ ê³„ì‚°**: BCE ì†ì‹¤ë¡œ í° ì˜¤ì°¨ ê°ì§€
4. **ê°€ì¤‘ì¹˜ ì¡°ì •**: ê¸ì • ë‹¨ì–´ë“¤ì˜ ê°€ì¤‘ì¹˜ ì¦ê°€, ë¶€ì • ë‹¨ì–´ë“¤ì˜ ê°€ì¤‘ì¹˜ ê°ì†Œ

#### ìµœì¢… ìƒíƒœ (í•™ìŠµ ì™„ë£Œ)
```
ë‹¨ì–´ë³„ ê°€ì¤‘ì¹˜: {"pizza": 0.8, "fantastic": 0.9, "terrible": -0.6, "good": 0.7, "bad": -0.5}
```

### 3. ëª¨ë¸ì˜ í•´ì„ ê°€ëŠ¥ì„±

ì´ ëª¨ë¸ì˜ ê°€ì¥ í° ì¥ì ì€ **í•´ì„ ê°€ëŠ¥ì„±**ì…ë‹ˆë‹¤:

- **ê¸ì • ë‹¨ì–´**: ì–‘ìˆ˜ ê°€ì¤‘ì¹˜ (fantastic: 0.9, good: 0.7)
- **ë¶€ì • ë‹¨ì–´**: ìŒìˆ˜ ê°€ì¤‘ì¹˜ (terrible: -0.6, bad: -0.5)
- **ì¤‘ë¦½ ë‹¨ì–´**: 0ì— ê°€ê¹Œìš´ ê°€ì¤‘ì¹˜

## ğŸ“ˆ ëª¨ë¸ ì„±ëŠ¥

### 1. í•™ìŠµ ì„¤ì •
- **ì†ì‹¤ í•¨ìˆ˜**: BCEWithLogitsLoss (ì´ì§„ ë¶„ë¥˜ ìµœì í™”)
- **ì˜µí‹°ë§ˆì´ì €**: Adam (learning_rate=0.001)
- **ë°°ì¹˜ í¬ê¸°**: 128
- **ì—í¬í¬**: 100 (ì¡°ê¸° ì¢…ë£Œ ì ìš©)

### 2. ì˜ˆì¸¡ í•¨ìˆ˜

**í•µì‹¬ ì˜ˆì¸¡ ì½”ë“œ:**
```python
def predict_rating(review, classifier, vectorizer, decision_threshold=0.5):
    """ë¦¬ë·°ì˜ ê°ì„±ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜"""
    # 1. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
    review = preprocess_text_spacy(review)
    
    # 2. ë²¡í„°í™”
    vectorized_review = torch.tensor(vectorizer.vectorize(review), dtype=torch.float32)
    
    # 3. ëª¨ë¸ ì˜ˆì¸¡
    result = classifier(vectorized_review.view(1, -1))
    probability_value = torch.sigmoid(result).item()
    
    # 4. ë¶„ë¥˜ ê²°ì •
    if probability_value < decision_threshold:
        return "Negative"
    else:
        return "Positive"
```

### 3. ì˜ˆì¸¡ ì˜ˆì‹œ

**ê¸ì • ë¦¬ë·°:**
```python
test_review = "this is a pretty awesome book"
prediction = predict_rating(test_review, classifier, vectorizer)
# ì²˜ë¦¬: awesome(0.8) + pretty(0.3) + good(0.7) = 1.8
# í™•ë¥ : ì‹œê·¸ëª¨ì´ë“œ(1.8) = 0.86 > 0.5
# ê²°ê³¼: "Positive"
```

**ë¶€ì • ë¦¬ë·°:**
```python
test_review = "terrible service and awful food"
prediction = predict_rating(test_review, classifier, vectorizer)
# ì²˜ë¦¬: terrible(-0.6) + awful(-0.4) + bad(-0.5) = -1.5
# í™•ë¥ : ì‹œê·¸ëª¨ì´ë“œ(-1.5) = 0.18 < 0.5
# ê²°ê³¼: "Negative"
```

## ğŸ¯ ëª¨ë¸ì˜ íŠ¹ì§•

### ì¥ì 
1. **ë‹¨ìˆœì„±**: ë³µì¡í•œ êµ¬ì¡° ì—†ì´ íš¨ê³¼ì ì¸ ë¶„ë¥˜
2. **í•´ì„ ê°€ëŠ¥ì„±**: ê°€ì¤‘ì¹˜ë¡œ ë‹¨ì–´ì˜ ì˜í–¥ë ¥ í™•ì¸ ê°€ëŠ¥
3. **ë¹ ë¥¸ í•™ìŠµ**: ë‹¨ì¼ ë ˆì´ì–´ë¡œ ë¹ ë¥¸ ìˆ˜ë ´
4. **ì•ˆì •ì„±**: ê³¼ì í•© ìœ„í—˜ ë‚®ìŒ

### í•œê³„
1. **ë‹¨ì–´ ìˆœì„œ ë¬´ì‹œ**: "not good"ê³¼ "good"ì„ êµ¬ë¶„í•˜ì§€ ëª»í•¨
2. **ë¬¸ë§¥ ë¶€ì¡±**: ë¬¸ì¥ì˜ ì „ì²´ì ì¸ ì˜ë¯¸ íŒŒì•… í•œê³„
3. **í¬ì†Œì„± ë¬¸ì œ**: ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ì¸í•œ ê³ ì°¨ì› ë²¡í„°

## ğŸš€ í™œìš© ë¶„ì•¼

- **ë¦¬ë·° ë¶„ì„**: ì˜¨ë¼ì¸ ì‡¼í•‘ëª°, ë ˆìŠ¤í† ë‘ ë¦¬ë·° ê°ì„± ë¶„ì„
- **ì†Œì…œ ë¯¸ë””ì–´**: íŠ¸ìœ„í„°, í˜ì´ìŠ¤ë¶ ê°ì„± ëª¨ë‹ˆí„°ë§
- **ê³ ê° ì„œë¹„ìŠ¤**: ê³ ê° í”¼ë“œë°± ìë™ ë¶„ë¥˜ ë° ìš°ì„ ìˆœìœ„ ì„¤ì •
- **ë§ˆì¼€íŒ…**: ì œí’ˆ/ì„œë¹„ìŠ¤ì— ëŒ€í•œ ê³ ê° ë°˜ì‘ ì‹¤ì‹œê°„ ë¶„ì„

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
emotion_classification_nlp/
â”œâ”€â”€ emotion_classification_analysis.ipynb  # ë©”ì¸ ë…¸íŠ¸ë¶
â”œâ”€â”€ emotion_classification_analysis.md     # ì´ ë¬¸ì„œ
â””â”€â”€ data/                                  # ë°ì´í„° íŒŒì¼ë“¤
```

---

*ì´ í”„ë¡œì íŠ¸ëŠ” PyTorchë¥¼ í™œìš©í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ì˜ ê¸°ë³¸ ì›ë¦¬ë¥¼ ë³´ì—¬ì£¼ëŠ” êµìœ¡ìš© ì˜ˆì œì…ë‹ˆë‹¤.*
